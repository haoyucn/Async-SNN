{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from network import MultiAttributeSpikingNetwork \n",
    "import pickle as pkl\n",
    "import global_var\n",
    "import time\n",
    "import numpy as np\n",
    "from custom_datasets.XOR_dataset import XOR_DataSet as Dataset\n",
    "\n",
    "logDir = './experiment-logs/'\n",
    "if not os.path.exists(logDir):\n",
    "    os.makedirs(logDir)\n",
    "\n",
    "logDir = './savedModels/'\n",
    "if not os.path.exists(logDir):\n",
    "    os.makedirs(logDir)\n",
    "\n",
    "\n",
    "def print_weights(neuron):\n",
    "    for d in neuron.dendrites:\n",
    "        print(d.synapticAttribute.postSynReceptorAmp)\n",
    "        \n",
    "def print_clearance(neuron):\n",
    "    for d in neuron.dendrites:\n",
    "        print(d.synapticAttribute.transmitterClearanceRate)\n",
    "\n",
    "\n",
    "def train(trails = 1, layerSizes = [2,3,1], epochesPerTries = 100, network = None, dataset = None, outputThreshold = 2, useAccumulativeOutput = True, debug = False, train = True, loadPath = None):\n",
    "    totalTime = 0\n",
    "    totalEpochs = 0\n",
    "    for t in range(trails):\n",
    "        hist = []\n",
    "        test_network = network\n",
    "        if test_network is None:\n",
    "            test_network = MultiAttributeSpikingNetwork(layerSizes, outputThreshold = outputThreshold)\n",
    "            if loadPath:\n",
    "                test_network.importParameters(loadPath)\n",
    "        startTime = time.time()\n",
    "        acc, convergeEpoch, errorSum, hist = test_network.run(dataset, epoches = epochesPerTries, train=train, useAccumulativeOutput=useAccumulativeOutput)\n",
    "        epochTimeUsed = time.time() - startTime\n",
    "        totalTime += epochTimeUsed\n",
    "        totalEpochs += convergeEpoch\n",
    "        \n",
    "        print('trail:', t, 'acc:', acc, 'convergeEpoch', convergeEpoch, 'timePerEpoch', epochTimeUsed / (convergeEpoch + 1), 'errorSum', errorSum, '\\t\\t\\t')\n",
    "    print('total Time:', totalTime, 'secPerEpoch', totalTime / totalEpochs)\n",
    "    return hist, test_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_datasets.XOR_dataset import XOR_DataSet as Dataset\n",
    "\n",
    "hist, test_network = train(trails = 10, layerSizes = [2,3,1], dataset = Dataset(), epochesPerTries = 800, useAccumulativeOutput = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delayed XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_datasets.XOR_dataset import Delayed_XOR_DataSet as Dataset\n",
    "\n",
    "hist, test_network = train(trails = 10, layerSizes = [2,3,1], dataset = Dataset(), epochesPerTries = 800, useAccumulativeOutput = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single channel XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_datasets.XOR_dataset import Single_Channel_XOR_DataSet_with_invert as Dataset\n",
    "\n",
    "hist, test_network = train(trails = 10, layerSizes = [1,3,1], dataset = Dataset(), epochesPerTries = 800, useAccumulativeOutput = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single channel with interruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_datasets.XOR_dataset import Single_Channel_XOR_DataSet_with_invert_with_interruption as Dataset\n",
    "\n",
    "hist, test_network = train(trails = 10, layerSizes = [1,3,1], dataset = Dataset(), epochesPerTries = 800, useAccumulativeOutput = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word completion test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from custom_datasets.word_completion_dataset import Words_Completion_small as Dataset\n",
    "\n",
    "hist,test_network = train(trails = 10, layerSizes =  [26, 72, 26], dataset = Dataset(),  epochesPerTries = 400, useAccumulativeOutput = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
